import os
import requests
import pandas as pd
import time
from datetime import datetime
from urllib.parse import quote

# Configuraci√≥n segura desde secretos
TOKEN = os.getenv("API_TOKEN")
BASE_URL = os.getenv("API_BASE_URL")
HEADERS = {"token": TOKEN}

# Configuraci√≥n modificada seg√∫n requerimientos
MAX_RETRIES = 0  # Sin reintentos
REQUEST_DELAY = 20  # 20 segundos entre p√°ginas
PAGE_SIZE = 10000  # 10,000 registros por p√°gina
TOTAL_ROWS = 700000  # Total estimado de registros

# ENDPOINTS - Solo mantener consulta1
ENDPOINTS = {
    "Consulta_1": "System.InventoryItemsSnap.List.View1"
}

# Configuraci√≥n de la consulta - Solo Consulta_1
QUERY_CONFIG = [
    {
        "name": "Consulta_1",
        "params": {
            "orderby": "civi_snapshot_date desc",
            "take": str(PAGE_SIZE),  # Usar PAGE_SIZE din√°mico
            "skip": 0,  # Inicializar skip en 0
        }
    }
]

def build_url(endpoint, params):
    param_parts = []
    for key, value in params.items():
        # Codificar valores para URL
        encoded_value = quote(str(value))
        param_parts.append(f"{key}={encoded_value}")
    url = f"{BASE_URL}{endpoint}?{'&'.join(param_parts)}"
    return url

def fetch_data_page(url, name, page_number):
    print(f"üìÑ Consultando p√°gina {page_number} para {name}")
    try:
        response = requests.get(url, headers=HEADERS, timeout=60)
        response.raise_for_status()
        data = response.json()
        
        if not data:
            print(f"‚ö†Ô∏è  P√°gina {page_number} de {name} no devolvi√≥ datos.")
            return None, False
            
        if isinstance(data, dict) and data.get("error"):
            print(f"‚ö†Ô∏è  Error en p√°gina {page_number} de {name}: {data.get('error')}")
            return None, False
            
        df = pd.json_normalize(data)
        df["load_timestamp"] = datetime.now().isoformat()
        df["page_number"] = page_number
        
        # Verificar si hay m√°s p√°ginas (si obtenemos menos registros de los solicitados)
        has_more_pages = len(df) == PAGE_SIZE
        return df, has_more_pages
        
    except requests.exceptions.RequestException as e:
        print(f"‚ö†Ô∏è  Error en p√°gina {page_number} de {name}: {e}")
        return None, False
    except ValueError as e:
        print(f"‚ö†Ô∏è  Error al decodificar JSON en p√°gina {page_number} de {name}: {e}")
        return None, False

def save_data_csv(df, name):
    os.makedirs("data", exist_ok=True)
    # Formato: Hist√≥rico_YYYY-MM-DD_HH-MM-SS.csv
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    filename = f"Historico_{timestamp}.csv"
    path = f"data/{filename}"
    df.to_csv(path, index=False, encoding='utf-8')
    print(f"üíæ Guardado: {path} - {len(df)} registros")
    return path

def main():
    print("üöÄ Iniciando consulta paginada para Hist√≥rico de Inventarios")
    start_time = time.time()

    all_data = pd.DataFrame()
    query = QUERY_CONFIG[0]  # Solo la primera consulta
    name = query["name"]
    page_number = 1
    
    has_more_pages = True
    
    while has_more_pages:
        # Construir URL con paginaci√≥n
        query_params = query["params"].copy()
        query_params["skip"] = (page_number - 1) * PAGE_SIZE
        
        url = build_url(ENDPOINTS[name], query_params)
        
        # Consultar p√°gina
        df_page, has_more_pages = fetch_data_page(url, name, page_number)
        
        if df_page is not None:
            # Concatenar datos
            all_data = pd.concat([all_data, df_page], ignore_index=True)
            print(f"üìä P√°gina {page_number}: {len(df_page)} registros - Total acumulado: {len(all_data)}")
            
            # Espera de 20 segundos entre p√°ginas
            if has_more_pages:
                print(f"‚è≥ Esperando {REQUEST_DELAY} segundos antes de la siguiente p√°gina...")
                time.sleep(REQUEST_DELAY)
        else:
            print(f"‚ùå Error en p√°gina {page_number}, deteniendo la consulta.")
            break
        
        page_number += 1
        
        # L√≠mite de seguridad (m√°ximo 70 p√°ginas para 700,000 registros)
        if page_number > 70:
            print("‚ö†Ô∏è  L√≠mite m√°ximo de p√°ginas alcanzado (70 p√°ginas)")
            break

    # Guardar todos los datos en CSV
    if not all_data.empty:
        file_path = save_data_csv(all_data, name)
        print(f"‚úÖ Proceso completado. Total de registros obtenidos: {len(all_data)}")
        print(f"üìÅ Archivo guardado como: {file_path}")
    else:
        print("‚ùå No se obtuvieron datos.")

    duration = time.time() - start_time
    print(f"‚è±Ô∏è  Tiempo total del proceso: {duration:.2f} segundos")

if __name__ == "__main__":
    main()
